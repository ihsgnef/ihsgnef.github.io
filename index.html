<!DOCTYPE html>
<!-- saved from url=(0029)http://www.shifeng.umiacs.io/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Required meta tags -->
    
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

    <!-- Icons -->
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/academicons.min.css">
    <link href="css/css2" rel="stylesheet">

    <title>Shi Feng</title>
  <style>undefined</style><link rel="preconnect" href="https://fonts.googleapis.com/" crossorigin="true"><link rel="preconnect" href="https://fonts.gstatic.com/"><link rel="stylesheet" href="./Shi Feng_files/css2(1)"></head>
  <body>
    <div class="container mt-5">
      <div class="row">
        <div class="col-md-5 offset-md-2">
          <a name="aboaut"><h3>Shi Feng</h3></a>
          <!-- shifeng@nyu.edu</br></br> -->

          <p>
          I am a postdoc in Computer Science at University Chicago working with 
          <a href="https://chenhaot.com">Chenhao Tan</a>.
          I got my PhD from University of Maryland advised by
          <a href="http://www.umiacs.umd.edu/~jbg/">Jordan Boyd-Graber</a>.
          </br></br>
          I work on AI interpretability and safety, with a focus on NLP models.
          Our recent <a href="https://xai-hcee.github.io/">tutorial</a>
          at NAACL gives a good overview for problems that I care about.
          </br></br>
          <strong>I'm on the job market this year.</strong>

          </p>
        </div>
        <div class="col-md-3">
        <img src="img/shifeng_headshot.jpg" alt="my face" class="img-fluid rounded">
        <a href="docs/shifeng_cv.pdf">[cv]</a>
        <a href="mailto:%20shif@uchicago.edu">[email]</a>
        <a href="https://scholar.google.com/citations?hl=en&amp;user=d0npq2oAAAAJ">[scholar]</a>
        <!-- a href="https://github.com/ihsgnef">[github]</a-->
        <!-- a href="https://twitter.com/ihsgnef">[twitter]</a -->
        </div>
      </div>
    </div>

    <div class="container pt-3">
      <div class="row">
        <div class="col-md-8 offset-md-2">
          <h3>Talks</h3>
          <ul>
              <li><strong>2023</strong>
              Pragmatic Machine Explanations <a href="docs/2023_job_shifeng.pdf">[pdf]</a>
              </li>
          </ul>
        </div>
      </div>
    </div>

    <div class="container pt-3">
      <div class="row">
        <div class="col-md-8 offset-md-2">
          <h3>Papers</h3>
          <ul>

              <li>
              <strong>Machine Explanations and Human Understanding</strong><br>
              Chacha Chen*, Shi Feng*, Amit Sharma, Chenhao Tan <br>
              <i>TMLR 2023</i>, <i>HMCaT @ ICML 2022, best paper</i>
              <a href="https://arxiv.org/abs/2202.04092">[arxiv]</a>
              <!-- a href="https://openreview.net/forum?id=y4CGF1A8VG">[openreview]</a -->
              <br>
              <!-- font color="#888888">TBD.</font -->
              </li><br>


              <li>
              <strong>Learning Human-Compatible Representations for Case-Based Decision Support</strong><br>
              Han Liu, Yizhou Tian, Chacha Chen, Shi Feng, Yuxin Chen, Chenhao Tan<br> 
              <i>ICLR 2023</i>
              <a href="https://openreview.net/forum?id=r0xte-t40I">[openreview]</a>
              <br>
              </li><br>

              <li>
              <strong>Pragmatic Machine Explanations</strong><br>
              Shi Feng, Chenhao Tan<br> 
              <i>To appear at HCAI@NeurIPS 2022</i>
              <a href="docs/2022_pragmatic_explanations_preprint.pdf">[pdf]</a>
              <br>
              </li><br>

              <li>
              <strong>Learning to Explain Selectively</strong><br>
              Shi Feng, Jordan Boyd-Graber<br>
              <i>EMNLP 2022</i>
              <a href="https://aclanthology.org/2022.emnlp-main.573">[acl]</a>
              <a href="https://aclanthology.org/2022.emnlp-main.573.pdf">[pdf]</a>
              <br>
              <!-- font color="#888888">TBD.</font -->
              </li><br>

              <li>
              <strong>Active Example Selection for In-Context Learning</strong><br>
              Yiming Zhang, Shi Feng, Chenhao Tan<br>
              <i>EMNLP 2022</i>
              <a href="https://aclanthology.org/2022.emnlp-main.622">[acl]</a>
              <a href="https://aclanthology.org/2022.emnlp-main.622.pdf">[pdf]</a>
              <br>
              <!-- font color="#888888">TBD.</font -->
              </li><br>

              <li>
              <strong>Human Learning Meets Representation Learning</strong><br>
              Matthew Shu*, Shi Feng*, Jordan Boyd-Graber<br>
              <i>preprint</i>
              <br>
              <!-- font color="#888888">TBD.</font -->
              </li><br>


              <li>
              <strong>Calibrate Before Use: Improving Few-shot Performance of Language Models</strong><br>
              Tony Z. Zhao*, Eric Wallace*, Shi Feng, Dan Klein, Sameer Singh<br>
              <i>ICML 2021</i>
              <a href="https://proceedings.mlr.press/v139/zhao21c.html">[pmlr]</a>
              <a href="http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf">[pdf]</a>
              <br>
              <!-- font color="#888888">A more effective way to use GPT-3/2.</font -->
              </li><br>

              <li>
              <strong>Concealed Data Poisoning Attacks on NLP Models</strong><br>
              Eric Wallace*, Tony Z. Zhao*, Shi Feng, Sameer Singh<br>
              <i>NAACL 2021</i>
              <a href="https://aclanthology.org/2021.naacl-main.13/">[acl]</a>
              <a href="https://aclanthology.org/2021.naacl-main.13.pdf">[pdf]</a>
              <a href="https://www.ericswallace.com/poisoning">[blog]</a>
              <br>
              <!-- font color="#888888">We devise a training data poisoning attack
                  that allows the adversary to create triggers.</font -->
              </li><br>

              <li>
              <strong>Universal Adversarial Triggers for Attacking and Analyzing NLP</strong><br>
              Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh<br>
              <i>EMNLP 2019</i>
              <a href="https://aclanthology.org/D19-1221/">[acl]</a>
              <a href="https://aclanthology.org/D19-1221.pdf">[pdf]</a>
              <a href="http://www.ericswallace.com/triggers">[blog]</a>
              <br>
              <!-- font color="#888888">We discover special phrases that causes
                  GPT-2 to generate toxic content whenever we append the phrase
                  to the beginning of a sentence. Similarly, we find phrases
                  that cause misclassification in reading comprehension,
                  natural language inference, and sentiment classification.</font -->
              </li><br>

              <li>
              <strong>Misleading Failures of Partial-input Baselines</strong><br>
              Shi Feng, Eric Wallace, Jordan Boyd-Graber<br>
              <i>ACL 2019, short paper</i>
              <a href="https://aclanthology.org/P19-1554/">[acl]</a>
              <a href="https://aclanthology.org/P19-1554.pdf">[pdf]</a>
              <br>
              <!-- font color="#888888">Partial-input baselines (e.g.,
                  hypothesis-only models for SNLI) are useful sanity
                  checks of dataset quality.  But what does it really mean
                  to pass theses checks? Are the "hard" examples we
                  identify really hard? We highlight potential pitfalls
                  when using these baselines for dataset quality control,
                  on both artificial and real datasets.</font -->
              </li><br>

              <li>
              <strong>Quizbowl: The Case for Incremental Question Answering</strong><br>
              Pedro Rodriguez, Shi Feng, Mohit Iyyer, He He, Jordan Boyd-Graber<br>
              <i>In submission</i> <a href="https://arxiv.org/abs/1904.04792">[arxiv]</a>
              <br>
              <!-- font color="#888888">
                  Quizbowl is a factoid QA dataset that challenges both
                  computers and humans in unique ways. It's also a
                  fruitful platform that lead to many exciting research
                  projects. This is our latest version of the definitive
                  guide to Quizbowl.
              </font -->
              </li><br>

              <li>
              <strong>Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation</strong><br>
              Sahil Singla, Eric Wallace, Shi Feng, Soheil Feizi<br>
              <i>ICML 2019</i>
              <a href="https://proceedings.mlr.press/v97/singla19a.html">[pmlr]</a>
              <a href="http://proceedings.mlr.press/v97/singla19a/singla19a.pdf">[pdf]</a>
              <br>
              <!-- font color="#888888">
                  Many model explanations use gradients and implicitly
                  make a local first-order approximation of the model.
                  Being approximations, they can't match the model
                  prefectly. We relax this first-order assumption and
                  enable the interpretation to capture inter-feature
                  dependencies. We also propose a method to efficiently
                  compute it for ReLU models and analyze when adding
                  high-order information helps.
              </font -->
              </li><br>

              <li>
              <strong>Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for <br>Question Answering</strong><br>
              Eric Wallace, Pedro Rodriguez, Shi Feng, Jordan Boyd-Graber<br>
              <i>TACL 2019</i>
              <a href="https://aclanthology.org/Q19-1029/">[acl]</a>
              <a href="https://aclanthology.org/Q19-1029.pdf">[pdf]</a><br>
              <!-- font color="#888888">
                  Crafting adversarial examples for NLP is difficult. Most
                  exicting work use limited perturbations to the input.
                  Can we create more diverse and less templated
                  adversarial examples? One direction we demonstrate in
                  this paper is to put a human in the loop. Importantly,
                  use model interpretations to guide the human writers.
              </font -->
              </li><br>

              <li>
              <strong>What can AI do for me: Evaluating Machine Learning Interpretations in Cooperative Play</strong><br>
              Shi Feng, Jordan Boyd-Graber<br>
              <i>IUI 2019</i>
              <a href="https://arxiv.org/abs/1810.09648">[arxiv]</a>
              <!-- a href="http://play.qanta.org/">[try it out!]</a -->
              <br>
              <!-- font color="#888888">
                  In many real world scenarios, the best way to deploy a
                  machine learning system is not to replace the humans
                  completely, but to have them work together. How can we
                  make this cooperation more effective? In this paper, we
                  explore how different intepretations assist this
                  cooperation with both experts and non-experts.
              </font -->
              </li><br>

              <li>
              <strong>Pathologies of Neural Models Make Interpretation Difficult</strong><br>
              Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber<br>
              <i>EMNLP 2018, oral</i>
              <!-- a href="https://arxiv.org/abs/1804.07781">[arxiv]</a-->
              <a href="https://aclanthology.org/D18-1407/">[acl]</a>
              <a href="https://aclanthology.org/D18-1407.pdf">[pdf]</a>
              <a href="https://vimeo.com/306158589">[talk]</a>
              <!-- a href="http://www.shifeng.umiacs.io/files/2018_emnlp_pathologies_slides.pdf">[slides]</a-->
              <br>
              <!-- font color="#888888">
                  Many interpretation methods in NLP derive feature
                  importance from model confidence. But there are known
                  issues regarding the confidence of a deep neural
                  network: DNNs without calibration show over-confidence;
                  they make high confidence incorrect predictions on
                  adversarial examples. How do these issues affect
                  confidence-based interpretation methods? We use <i>input
                  reduction</i> to expose pathological model predictions
                  and (partly) answer this question.
              </font -->
              </li><br>

              <li>
              <strong>Interpreting Neural Networks with Nearest Neighbors</strong><br>
              Eric Wallace*, Shi Feng*, Jordan Boyd-Graber<br>
              <i>BlackboxNLP @ EMNLP 2018</i>
              <a href="https://aclanthology.org/W18-5416/">[acl]</a>
              <a href="https://aclanthology.org/W18-5416.pdf">[pdf]</a>
		  	  <!-- a href="https://zerobatchsize.net/2018/09/11/dknn.html">[blog]</a -->
              <br>
              <!-- font color="#888888">
                  Can we create inherently interpretable NLP models? We
                  show this is possible on various task. It turns out you
                  can replace the softmax layer with deep k-nearest
                  neighbor search without sacrificing accuracy, and the
                  model is inherently interpretable.
              </font -->
              </li><br>

              <li>
              <strong>The UMD Neural Machine Translation Systems at WMT17 Bandit Learning Task</strong><br>
              Amr Sharaf, Shi Feng, Khanh Nguyen, Kianté Brantley, Hal Daumé III<br>
              <i>WMT @ EMNLP 2017</i>
              <a href="https://aclanthology.org/W17-4778/">[acl]</a>
              <a href="https://aclanthology.org/W17-4778.pdf">[pdf]</a>
              <br>
              <!-- font color="#888888">
                  Can you learn to adapt a machine translation system
                  without having ground-truth translation pairs from the
                  target domain? We show how to use reinforcement learning
                  algorithms to adapt a MT system using weak feedback.
              </font -->
              </li><br>

              <li>
              <strong>Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation</strong><br>
              Shi Feng, Shujie Liu, Nan Yang, Mu Li, Ming Zhou, Kenny Q. Zhu<br>
              <i>COLING 2016</i>
		  	<a href="https://aclanthology.org/C16-1290/">[acl]</a>
		  	<a href="https://aclanthology.org/C16-1290.pdf">[pdf]</a>
            <br>
              <!-- font color="#888888">
                  One of the first attempts at adding structure and
                  inductive biases to the attention mechanism for machine
                  translation.
              </font -->
              </li><br>
          </ul>
      </div>
    </div>

    <div class="container pt-3">
      <div class="row">
        <div class="col-md-8 offset-md-2">
          <h3>Experience</h3>
          <ul>
              <li><strong>June 1 2020</strong> Intern @ Salesforce Research</li>
              <li><strong>Apr 25 2019</strong> <a href="https://soundcloud.com/nlp-highlights/87-pathologies-of-neural-models-make-interpretation-difficult-with-shi-feng">NLP Highlights Podcast</a> on interpretation of NLP models</li>
              <li><strong>Mar 2019</strong> Invited talks at UPenn, UCSD, UCI</li>
              <li><strong>Best reviewer award</strong> @ EMNLP 2018, 2020</li>
              <li><strong>Summer 2018</strong> Research Intern @ Microsoft Research</li>
          </ul>
        </div>
      </div>
    </div>



    <!-- div class="container pt-3">
      <div class="row">
        <div class="col-md-8 offset-md-2">
          <h3>Service</h3>
          <ul>
              <li><strong>Reviewer</strong> EMNLP'18/19, AAAI'20</li>
          </ul>
        </div>
      </div>
    </div-->

</div></body></html>
