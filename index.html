<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Required meta tags -->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<!-- Bootstrap CSS -->
<link rel="stylesheet" href="css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<!-- Icons -->
<link rel="stylesheet" href="css/font-awesome.min.css">
<link rel="stylesheet" href="css/academicons.min.css">
<link href="css/css2" rel="stylesheet">

<!-- Fonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:ital,wght@0,400;0,600;1,400;1,600&display=swap" rel="stylesheet">

<link rel="stylesheet" href="./css/style.css">

<title>Shi Feng</title>
</head>

<body>
<div class="container pt-3">
<div class="row">
<div class="col-md-7 offset-md-2">

<a href="index.html">[about]</a> &nbsp;
<a href="#talks">[talks]</a> &nbsp;
<a href="#papers">[papers]</a> &nbsp;
</br></br>

<img src="img/shifeng_headshot.jpg" alt="my face" class="img-fluid rounded" width="250" height="250">
</br></br>
<h3>Shi Feng</h3>
<a href="mailto:%20shifeng@nyu.edu">[email]</a>
<a href="https://scholar.google.com/citations?hl=en&amp;user=d0npq2oAAAAJ">[scholar]</a>
<!-- a href="https://github.com/ihsgnef">[github]</a-->
<a href="docs/shifeng_cv.pdf">[cv]</a>
<a href="https://twitter.com/ihsgnef">[twitter]</a>
</br>Assistant professor at George Washington University.</br>
<!-- /br>Postdoc with Sam Bowman and He He at <a href="https://wp.nyu.edu/arg/">NYU Alignment Research Group</a>
</br>Postdoc with <a href="https://chenhaot.com">Chenhao Tan</a> at University of Chicago
</br>PhD with <a href="http://users.umiacs.umd.edu/~jbg/">Jordan Boyd-Graber</a> at University of Maryland
</br --></br>

<!--
I'm on the job market for academic and industry positions.</br>
Please refer to my 
<a href="docs/shifeng_research.pdf">research</a>,
<a href="docs/shifeng_teaching.pdf">teaching</a>, and
<a href="docs/shifeng_diversity.pdf">DEI</a> statements.
</br>
-->

I work on <b>AI safety</b>, in particular improving human supervision / scalable oversight.
</br>
<strong>Crowdsourcing truth at scale</strong> has been a main driving force behind the recent AI advancements, but AIs are quickly evolving beyond the paradigm where resource-constrained crowd workers can reliably provide supervision. My approach to this problem is to design <strong>truth-finding processes</strong> for and with AIs.
</br>
<!-- div style="margin-bottom:0.3cm;margin-top:0.3cm;margin-left:1cm;"></div>
To build a mental picture of what that looks like, I find it useful to think about <a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol#Game_2">AlphaGo's move 37</a>.
Many Go players initially dismissed it as a bug, but its value became apparent as the game unfolded.
In other words, even domain experts initially misjudged the AI, but the interactive process of Lee Sedol playing out the rest of the game lead to a more precise evaluation.</br>

<div style="margin-bottom:0.3cm;margin-top:0.3cm;margin-left:1cm;"></div-->

<!--> My high-level goal is to improve <strong>human oversight of AI systems</strong> with new <a href="https://arxiv.org/abs/2202.04092">theories</a>, <a href="https://aclanthology.org/2022.emnlp-main.573/">algorithms</a>, <a href="https://aclanthology.org/Q19-1029/">data collection schemes</a>, and <a href="https://arxiv.org/abs/1810.09648">UIs</a>. <-->

Recent research threads:

<ul>
    <li><strong>Meta-analysis of scalable oversight methods and evals</strong>: <a href="https://arxiv.org/abs/2404.13076">self-recognition</a>, <a href="https://arxiv.org/abs/2407.04549">in-context reward hacking</a>, <a href="https://arxiv.org/abs/2409.12822">unintended reward hacking</a>  </li>
    <li><strong>Personalizing LMs for education</strong> <a href="https://arxiv.org/abs/2402.12291">flashcards</a>, <a href="https://arxiv.org/abs/2406.15352">mnemonics</a></li>
</ul>

<div style="margin-bottom:0.8cm;"></div>

<!-- a id="talks"><h3>Talks</h3></a>
<ul>
    <li><strong>Sep 2024</strong> Challenges in AI-Assisted AI Supervision <a href="docs/2024_fall_supervision_v1.pdf">[pdf]</a></li>
    <li><strong>May 2023</strong> Evaluating AI: From Crowdsourcing Truths to Truth-finding Processes <a href="docs/2023_job_v9.pdf">[pdf]</a></li>
    <li><strong>Jul 2022</strong> NAACL Tutorial on Human Evaluations of Explanations <a href="https://xai-hcee.github.io/">[website]</a></li>
    <li><strong>Apr 2019</strong> NLP Highlights Podcast on pathologies of neural models <a href="https://open.spotify.com/episode/0SZpHmSrn3B9aK0uWPSv8r?si=8885e60ab8c1418f">[spotify]</a></li>

</ul>
<div style="margin-bottom:0.8cm;"></div -->

<a id="papers"><h3>Papers</h3></a>
<ul>

<li>
<strong>Language Models Learn to Mislead Humans via RLHF</strong><br>
Jiaxin Wen, Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel R. Boman, He He, <u>Shi Feng</u></br>
<a href="https://arxiv.org/abs/2409.12822">[arxiv]</a>
<br>
</li><br>


<li>
<strong>Spontaneous Reward Hacking in Iterative Self-Refinement</strong><br>
Jane Pan, He He, Samuel R. Bowman, <u>Shi Feng</u></br>
<a href="https://arxiv.org/abs/2407.04549">[arxiv]</a>
<br>
</li><br>

<li>
<strong>LLM Evaluators Recognize and Favor Their Own Generations</strong><br>
Arjun Panickssery, Samuel R. Bowman, <u>Shi Feng</u></br>
<i>NeurIPS 2024, oral</i>
<a href="https://arxiv.org/abs/2404.13076">[arxiv]</a>
<br>
</li><br>

<li>
<strong>Large Language Models Help Humans Verify Truthfulness—Except When They Are Convincingly Wrong</strong><br>
Chenglei Si, Navita Goyal, Sherry Wu, Chen Zhao, <u>Shi Feng</u>, Hal Daumé III, Jordan Boyd-Graber </br>
<i>NAACL 2024</i>
<a href="https://arxiv.org/abs/2310.12558">[arxiv]</a>
<br>
</li><br>

<li>
<strong>KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students</strong></br>
Matt Shu, Nishant Balepur, <u>Shi Feng</u>, Jordan Boyd-Graber</br>
<i>EMNLP 2024</i>
<a href="https://arxiv.org/abs/2402.12291">[arxiv]</a>
<br>
</li><br>

<li>
<strong>A SMART Mnemonic Sounds like "Glue Tonic": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick</strong></br>
Nishant Balepur, Matt Shu, Alexander Hoyle, Alison Robey, <u>Shi Feng</u>, Seraphina Goldfarb-Tarrant, Jordan Boyd-Graber</br>
<i>EMNLP 2024</i>
<a href="https://arxiv.org/abs/2406.15352">[arxiv]</a>
<br>
</li><br>

<li>
<strong>Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations</strong><br>
Chenglei Si*, Dan Friedman*, Nitish Joshi, <u>Shi Feng</u>, Danqi Chen, He He </br>
<i>ACL 2023</i>
<a href="https://aclanthology.org/2023.acl-long.632/">[acl]</a>
<br>
</li><br>

<li>
<strong>Machine Explanations and Human Understanding</strong><br>
Chacha Chen*, <u>Shi Feng</u>*, Amit Sharma, Chenhao Tan <br>
<i>TMLR 2023</i>, <i>FAccT 2023</i>, and best paper at <i> HMCaT @ ICML 2022</i>
<a href="https://arxiv.org/abs/2202.04092">[arxiv]</a>
<!-- a href="https://openreview.net/forum?id=y4CGF1A8VG">[openreview]</a -->
<br>
<!-- font color="#888888">Theoretical necessity of pragmatics.</font -->
</li><br>


<li>
<strong>Learning Human-Compatible Representations for Case-Based Decision Support</strong><br>
Han Liu, Yizhou Tian, Chacha Chen, <u>Shi Feng</u>, Yuxin Chen, Chenhao Tan<br> 
<i>ICLR 2023</i>
<a href="https://openreview.net/forum?id=r0xte-t40I">[openreview]</a>
<br>
</li><br>

<!-- li>
<strong>Pragmatic Machine Explanations</strong><br>
<u>Shi Feng</u>, Chenhao Tan<br> 
<i>HCAI@NeurIPS 2022</i>
<a href="docs/2022_pragmatic_explanations_preprint.pdf">[pdf]</a>
<br>
</li><br -->

<li>
<strong>Learning to Explain Selectively</strong><br>
<u>Shi Feng</u>, Jordan Boyd-Graber<br>
<i>EMNLP 2022</i>
<a href="https://aclanthology.org/2022.emnlp-main.573">[acl]</a>
<a href="https://aclanthology.org/2022.emnlp-main.573.pdf">[pdf]</a>
<br>
<!-- font color="#888888">TBD.</font -->
</li><br>

<li>
<strong>Active Example Selection for In-Context Learning</strong><br>
Yiming Zhang, <u>Shi Feng</u>, Chenhao Tan<br>
<i>EMNLP 2022</i>
<a href="https://aclanthology.org/2022.emnlp-main.622">[acl]</a>
<a href="https://aclanthology.org/2022.emnlp-main.622.pdf">[pdf]</a>
<br>
<!-- font color="#888888">TBD.</font -->
</li><br>

<li>
<strong>Calibrate Before Use: Improving Few-shot Performance of Language Models</strong><br>
Tony Z. Zhao*, Eric Wallace*, <u>Shi Feng</u>, Dan Klein, Sameer Singh<br>
<i>ICML 2021</i>
<a href="https://proceedings.mlr.press/v139/zhao21c.html">[pmlr]</a>
<a href="http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf">[pdf]</a>
<br>
<!-- font color="#888888">A more effective way to use GPT-3/2.</font -->
</li><br>

<li>
<strong>Concealed Data Poisoning Attacks on NLP Models</strong><br>
Eric Wallace*, Tony Z. Zhao*, <u>Shi Feng</u>, Sameer Singh<br>
<i>NAACL 2021</i>
<a href="https://aclanthology.org/2021.naacl-main.13/">[acl]</a>
<a href="https://aclanthology.org/2021.naacl-main.13.pdf">[pdf]</a>
<a href="https://www.ericswallace.com/poisoning">[blog]</a>
<br>
<!-- font color="#888888">We devise a training data poisoning attack
    that allows the adversary to create triggers.</font -->
</li><br>

<li>
<strong>Universal Adversarial Triggers for Attacking and Analyzing NLP</strong><br>
Eric Wallace, <u>Shi Feng</u>, Nikhil Kandpal, Matt Gardner, Sameer Singh<br>
<i>EMNLP 2019, oral</i>
<a href="https://aclanthology.org/D19-1221/">[acl]</a>
<a href="https://aclanthology.org/D19-1221.pdf">[pdf]</a>
<a href="http://www.ericswallace.com/triggers">[blog]</a>
<br>
<!-- font color="#888888">We discover special phrases that causes
    GPT-2 to generate toxic content whenever we append the phrase
    to the beginning of a sentence. Similarly, we find phrases
    that cause misclassification in reading comprehension,
    natural language inference, and sentiment classification.</font -->
</li><br>

<li>
<strong>Misleading Failures of Partial-input Baselines</strong><br>
<u>Shi Feng</u>, Eric Wallace, Jordan Boyd-Graber<br>
<i>ACL 2019, short paper</i>
<a href="https://aclanthology.org/P19-1554/">[acl]</a>
<a href="https://aclanthology.org/P19-1554.pdf">[pdf]</a>
<br>
<!-- font color="#888888">Partial-input baselines (e.g.,
    hypothesis-only models for SNLI) are useful sanity
    checks of dataset quality.  But what does it really mean
    to pass theses checks? Are the "hard" examples we
    identify really hard? We highlight potential pitfalls
    when using these baselines for dataset quality control,
    on both artificial and real datasets.</font -->
</li><br>

<li>
<strong>Quizbowl: The Case for Incremental Question Answering</strong><br>
Pedro Rodriguez, <u>Shi Feng</u>, Mohit Iyyer, He He, Jordan Boyd-Graber<br>
<i>In submission</i> <a href="https://arxiv.org/abs/1904.04792">[arxiv]</a>
<br>
<!-- font color="#888888">
    Quizbowl is a factoid QA dataset that challenges both
    computers and humans in unique ways. It's also a
    fruitful platform that lead to many exciting research
    projects. This is our latest version of the definitive
    guide to Quizbowl.
</font -->
</li><br>

<li>
<strong>Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation</strong><br>
Sahil Singla, Eric Wallace, <u>Shi Feng</u>, Soheil Feizi<br>
<i>ICML 2019</i>
<a href="https://proceedings.mlr.press/v97/singla19a.html">[pmlr]</a>
<a href="http://proceedings.mlr.press/v97/singla19a/singla19a.pdf">[pdf]</a>
<br>
<!-- font color="#888888">
    Many model explanations use gradients and implicitly
    make a local first-order approximation of the model.
    Being approximations, they can't match the model
    prefectly. We relax this first-order assumption and
    enable the interpretation to capture inter-feature
    dependencies. We also propose a method to efficiently
    compute it for ReLU models and analyze when adding
    high-order information helps.
</font -->
</li><br>

<li>
<strong>Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering</strong><br>
Eric Wallace, Pedro Rodriguez, <u>Shi Feng</u>, Jordan Boyd-Graber<br>
<i>TACL 2019</i>
<a href="https://aclanthology.org/Q19-1029/">[acl]</a>
<a href="https://aclanthology.org/Q19-1029.pdf">[pdf]</a><br>
<!-- font color="#888888">
    Crafting adversarial examples for NLP is difficult. Most
    exicting work use limited perturbations to the input.
    Can we create more diverse and less templated
    adversarial examples? One direction we demonstrate in
    this paper is to put a human in the loop. Importantly,
    use model interpretations to guide the human writers.
</font -->
</li><br>

<li>
<strong>What can AI do for me: Evaluating Machine Learning Interpretations in Cooperative Play</strong><br>
<u>Shi Feng</u>, Jordan Boyd-Graber<br>
<i>IUI 2019</i>
<a href="https://arxiv.org/abs/1810.09648">[arxiv]</a>
<!-- a href="http://play.qanta.org/">[try it out!]</a -->
<br>
<!-- font color="#888888">
    In many real world scenarios, the best way to deploy a
    machine learning system is not to replace the humans
    completely, but to have them work together. How can we
    make this cooperation more effective? In this paper, we
    explore how different intepretations assist this
    cooperation with both experts and non-experts.
</font -->
</li><br>

<li>
<strong>Pathologies of Neural Models Make Interpretation Difficult</strong><br>
<u>Shi Feng</u>, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber<br>
<i>EMNLP 2018, oral</i>
<!-- a href="https://arxiv.org/abs/1804.07781">[arxiv]</a-->
<a href="https://aclanthology.org/D18-1407/">[acl]</a>
<a href="https://aclanthology.org/D18-1407.pdf">[pdf]</a>
<a href="https://vimeo.com/306158589">[talk]</a>
<!-- a href="http://www.shifeng.umiacs.io/files/2018_emnlp_pathologies_slides.pdf">[slides]</a-->
<br>
<!-- font color="#888888">
    Many interpretation methods in NLP derive feature
    importance from model confidence. But there are known
    issues regarding the confidence of a deep neural
    network: DNNs without calibration show over-confidence;
    they make high confidence incorrect predictions on
    adversarial examples. How do these issues affect
    confidence-based interpretation methods? We use <i>input
    reduction</i> to expose pathological model predictions
    and (partly) answer this question.
</font -->
</li><br>

<li>
<strong>Interpreting Neural Networks with Nearest Neighbors</strong><br>
Eric Wallace*, <u>Shi Feng</u>*, Jordan Boyd-Graber<br>
<i>BlackboxNLP @ EMNLP 2018</i>
<a href="https://aclanthology.org/W18-5416/">[acl]</a>
<a href="https://aclanthology.org/W18-5416.pdf">[pdf]</a>
  <!-- a href="https://zerobatchsize.net/2018/09/11/dknn.html">[blog]</a -->
<br>
<!-- font color="#888888">
    Can we create inherently interpretable NLP models? We
    show this is possible on various task. It turns out you
    can replace the softmax layer with deep k-nearest
    neighbor search without sacrificing accuracy, and the
    model is inherently interpretable.
</font -->
</li><br>

<li>
<strong>The UMD Neural Machine Translation Systems at WMT17 Bandit Learning Task</strong><br>
Amr Sharaf, <u>Shi Feng</u>, Khanh Nguyen, Kianté Brantley, Hal Daumé III<br>
<i>WMT @ EMNLP 2017</i>
<a href="https://aclanthology.org/W17-4778/">[acl]</a>
<a href="https://aclanthology.org/W17-4778.pdf">[pdf]</a>
<br>
<!-- font color="#888888">
    Can you learn to adapt a machine translation system
    without having ground-truth translation pairs from the
    target domain? We show how to use reinforcement learning
    algorithms to adapt a MT system using weak feedback.
</font -->
</li><br>

<li>
<strong>Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation</strong><br>
<u>Shi Feng</u>, Shujie Liu, Nan Yang, Mu Li, Ming Zhou, Kenny Q. Zhu<br>
<i>COLING 2016</i>
  <a href="https://aclanthology.org/C16-1290/">[acl]</a>
  <a href="https://aclanthology.org/C16-1290.pdf">[pdf]</a>
<br>
  <!-- font color="#888888">
      One of the first attempts at adding structure and
      inductive biases to the attention mechanism for machine
      translation.
  </font -->
</li>
</ul>
<div style="margin-bottom:0.8cm;"></div>

<h3>Experience</h3>
<ul>
    <li><strong>June 1 2020</strong> Intern @ Salesforce Research</li>
    <li><strong>Mar 2019</strong> Invited talks at UPenn, UCSD, UCI</li>
    <li><strong>Best reviewer award</strong> @ EMNLP 2018, 2020</li>
    <li><strong>Summer 2018</strong> Research Intern @ Microsoft Research</li>
</ul>
</div>
</div>
</div>
</div>
</body>
</html>
